{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import librosa, librosa.display\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import colorednoise as cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From here it's mostly creating a root directory from the different sources of data and combining them and their subfolders etc\n",
    "#### So I will not show it since it involves local directories and traversing subfolders etc. \n",
    "\n",
    "### What I will bring up\n",
    "- I am using 4 sources of data (Imagimob, Common Voice, ESC-50, and COVID-19 coughs)\n",
    "- Imagimob: 452 samples (chunks) | Coughvid: 1569 samples | The rest is Common Voice and ESC-50 (no cough sounds)\n",
    "- Cough: 2001 samples | No cough: 1979 samples (BALANCED!)\n",
    "- Data augmentation: 7962 samples in total which comes solely from inject noise into original samples (violating Nyquists theorem doesn't matter here since we're interested in model robustness)\n",
    "- Cutting or padding audio into 7 second files\n",
    "- Mel Spectrograms, Takens Embeddings, PCA, and 3D Point clouds | These are the features engineered\n",
    "\n",
    "#### Things in the whole deliverable but not in this notebook:\n",
    "- Monte Carlo dropout\n",
    "- k-fold cross validation\n",
    "- random subsampling\n",
    "- weight resetting to avoid weight/parameter leakage \n",
    "- weight initialization\n",
    "- hybrid CNN with 3D and 2D input\n",
    "- taking 1D data, transforming it into 2D, dimensionality reduction into 3 features, and then slicing it such that it becomes 3D with a tensor\n",
    "- **unit tests**\n",
    "- terminal/shell training with arguments\n",
    "\n",
    "\n",
    "#### Below are examples of how it has been done. See the other companion notebooks for fun stuff. <br>\n",
    "\n",
    "**FYI**: Statistical data analysis has not been included because the task is not targeted at inferential statistics and the dataset bears no interesting statistical usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"C:/Users/jako/data/Imagimob_ML_test/\"\n",
    "\n",
    "catg = os.path.join(dataset_path, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [subfolder for subfolder in os.listdir(catg) if os.path.isdir(os.path.join(catg, subfolder))]\n",
    "\n",
    "audio_path = os.path.join(catg, \"audio\")\n",
    "metadata_path = os.path.join(catg, 'metadata')\n",
    "coughs_path = os.path.join(audio_path, \"coughs\")\n",
    "no_coughs_path = os.path.join(audio_path, \"no_coughs\")\n",
    "\n",
    "os.makedirs(audio_path, exist_ok=True)\n",
    "os.makedirs(metadata_path, exist_ok=True)\n",
    "os.makedirs(coughs_path, exist_ok=True)\n",
    "os.makedirs(no_coughs_path, exist_ok=True)\n",
    "\n",
    "for category in cc:\n",
    "    catpath = os.path.join(catg, category)\n",
    "    \n",
    "    sample_folders = [sampfold for sampfold in os.listdir(catpath) if os.path.isdir(os.path.join(catpath, sampfold))]\n",
    "    \n",
    "    \n",
    "    if category in [\"coughing\", \"coughing_batch_2\"]:\n",
    "        dest_audio_path = coughs_path\n",
    "    else:\n",
    "        dest_audio_path = no_coughs_path\n",
    "        \n",
    "    for sampfold in sample_folders:\n",
    "        sample_folder_path = os.path.join(catpath, sampfold)\n",
    "        \n",
    "        for fname in os.listdir(sample_folder_path):\n",
    "            \n",
    "            \n",
    "            filepath = os.path.join(sample_folder_path, fname)\n",
    "            \n",
    "            new_fname = category + \"_\" + sampfold + \"_\" + fname\n",
    "            \n",
    "            if fname.endswith(\".wav\"):\n",
    "                dest_path = os.path.join(dest_audio_path, new_fname)\n",
    "            elif fname.endswith(\".label\"):\n",
    "                dest_path = os.path.join(metadata_path, new_fname)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "            shutil.move(filepath, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid adding complexity because of a subfolder inside a subfolder    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mictap = os.path.join(catg, \"mic_tapping\")\n",
    "micc = [subfolder for subfolder in os.listdir(mictap) if os.path.isdir(os.path.join(mictap, subfolder))]\n",
    "\n",
    "for category in micc:\n",
    "    catpath = os.path.join(mictap, category)\n",
    "    \n",
    "    sample_folders = [sampfold for sampfold in os.listdir(catpath) if os.path.isdir(os.path.join(catpath, sampfold))]\n",
    "\n",
    "    dest_audio_path = no_coughs_path\n",
    "\n",
    "    for sampfold in sample_folders:\n",
    "        sample_folder_path = os.path.join(catpath, sampfold)\n",
    "        \n",
    "        for fname in os.listdir(sample_folder_path):\n",
    "            \n",
    "            \n",
    "            filepath = os.path.join(sample_folder_path, fname)\n",
    "            \n",
    "            new_fname = \"mic_tapping\" + \"_\" + sampfold + \"_\" + fname\n",
    "            \n",
    "            if fname.endswith(\".wav\"):\n",
    "                \n",
    "                dest_path = os.path.join(dest_audio_path, new_fname)\n",
    "            elif fname.endswith(\".label\"):\n",
    "                dest_path = os.path.join(metadata_path, new_fname)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "            shutil.move(filepath, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"C:/Users/jako/data/Imagimob_ML_test/data/audio/\"\n",
    "LABEL_PATH = \"C:/Users/jako/data/Imagimob_ML_test/data/metadata/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cough_wav_list = cough_wav_list = [file for file in os.listdir(os.path.join(AUDIO_PATH,\"coughs\")) if file.endswith(\".wav\") ]\n",
    "no_cough_wav_list = [file for file in os.listdir(os.path.join(AUDIO_PATH,\"no_coughs\")) if file.endswith(\".wav\") ]\n",
    "\n",
    "full_lbl_list = os.listdir(LABEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .wav files: 70\n",
      "Number of .label files: 70\n"
     ]
    }
   ],
   "source": [
    "NO_WAV_FILES = len(cough_wav_list) + len(no_cough_wav_list)\n",
    "print(\"Number of .wav files: {}\".format(NO_WAV_FILES))\n",
    "print(\"Number of .label files: {}\".format(len(full_lbl_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scan through the raw audio files and check their length statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_file_coughs = []\n",
    "helper_fname = os.path.join(AUDIO_PATH, \"coughs\") + \"/\"\n",
    "\n",
    "for wavfile in os.listdir(os.path.join(AUDIO_PATH, \"coughs\")):\n",
    "    if wavfile.endswith(\".wav\"):\n",
    "        y, sr = librosa.load(helper_fname + wavfile, sr=None)\n",
    "        soundlength = len(y)/sr\n",
    "        length_of_file_coughs.append( soundlength )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length in seconds: 42.6 seconds\n",
      "Median length in seconds: 61.08 seconds\n",
      "Standard deviation of the sample lengths: 27.02 seconds\n",
      "The 95th percentile of sample lengths: 70.824 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Average length in seconds: {} seconds\".format( round(np.mean(length_of_file_coughs), 2)))\n",
    "print(\"Median length in seconds: {} seconds\".format( np.median(length_of_file_coughs) ))\n",
    "print(\"Standard deviation of the sample lengths: {} seconds\".format( round(np.std(length_of_file_coughs), 2)))\n",
    "print(\"The 95th percentile of sample lengths: {} seconds\".format( np.quantile(length_of_file_coughs, 0.95) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't work with different lengths of data. How about splitting them into 5 second chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_7(audio_array, samplerate, wav_name, output_dir):\n",
    "    \n",
    "    sample_chunk = 5 * samplerate\n",
    "    \n",
    "    numchunks = len(audio_array) // sample_chunk # Integer division to avoid exceeding length in forloop\n",
    "    \n",
    "    criteria = len(audio_array) // samplerate\n",
    "    \n",
    "    if criteria > 5*2:\n",
    "        \n",
    "        for i in range(numchunks):\n",
    "            \n",
    "            chunk = audio_array[i*sample_chunk:(i+1)*sample_chunk]\n",
    "            new_name = wav_name[:-4]\n",
    "            output_path = os.path.join(output_dir, f\"{new_name}_chunk_{i+1}.wav\")\n",
    "            sf.write(output_path, chunk, samplerate)\n",
    "\n",
    "    else:\n",
    "        print(\"Criteria of 5 seconds not met. Minimum must be 10 seconds.\")\n",
    "\n",
    "\n",
    "\n",
    "def process_audio_files(directory, base_path, output_subfolder):\n",
    "    \"\"\"\n",
    "    Process audio files in the given directory.\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(base_path, directory)\n",
    "    output_path = os.path.join(base_path, directory, output_subfolder)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    length_of_file = []\n",
    "    for wavfile in os.listdir(input_path):\n",
    "        file_path = os.path.join(input_path, wavfile)\n",
    "        \n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        soundlength = len(y) / sr\n",
    "        length_of_file.append(soundlength)\n",
    "        \n",
    "        split_to_7(y, sr, wavfile, output_path)\n",
    "    \n",
    "    return length_of_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING: Cutting or padding\n",
    "\n",
    "Based on exploratory data analysis where skewness of the length distribution shows that the large majority are concentrated around 9 seconds. <br>\n",
    "Choosing 7 seconds to keep everything computationally efficient but also practically valuable. <br>\n",
    "Exploratory analysis not added because the task is not about statistical analysis and the data provides no interesting statistical usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_when_needed(signal):\n",
    "    if len(signal) > (7*16000):\n",
    "        signal = signal[:7*16000]\n",
    "    else:\n",
    "        signal = signal\n",
    "    return signal\n",
    "\n",
    "def stretch_when_needed(signal):\n",
    "    signal_length = len(signal)\n",
    "    if signal_length < (7*16000):\n",
    "        num_missing = (7*16000) - signal_length\n",
    "        last = (0, num_missing)\n",
    "        signal = torch.nn.functional.pad(torch.tensor(signal), last)\n",
    "    else:\n",
    "        signal = signal\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING: Mel Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = [\"/data/custom_cough/cough\", \"/data/custom_cough/no_cough\"]\n",
    "dest_path = \"/data/custom_cough/mels\"\n",
    "\n",
    "meller = MelSpectrogram(16000, n_fft=1024, hop_length=512, n_mels=64)\n",
    "\n",
    "\n",
    "# Using Polars library\n",
    "for group in dataframe[\"label\"].unique():\n",
    "    data = dataframe.filter(pl.col(\"label\") == group)\n",
    "    \n",
    "    for row in data[\"id\"]:\n",
    "        if group == 1:\n",
    "            fullpath = os.path.join(wav_paths[0], row)\n",
    "        else:\n",
    "            fullpath = os.path.join(wav_paths[1], row)\n",
    "\n",
    "        y, sr = librosa.load(fullpath, sr=None)\n",
    "        y = cut_when_needed(y)                      # Apply cutting if signal too long\n",
    "        y = stretch_when_needed(y)                  # Apply right padding if signal too short\n",
    "        melconvert = meller(torch.tensor(y))        # Convert into mel spectrogram\n",
    "        newname = row[:-4] + \".pt\"          # Use .pt format\n",
    "        torch.save(melconvert, os.path.join(dest_path, newname)) # Save as a torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING: Create a noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_some_noise(beta: int = 1, freq: int = 44100) -> np.array:\n",
    "  \"\"\"\n",
    "  Function to generate some noise (defaults to pink noise with beta=1)\n",
    "  beta = 1 is for pink noise and is standard, 0 is Gaussian\n",
    "  freq =  is just the number of samples \n",
    "  \"\"\"\n",
    "  noise_array = cn.powerlaw_psd_gaussian(beta, freq)      # Generate the noise\n",
    "  noise_array = (noise_array - np.min(noise_array)) / (np.max(noise_array) - np.min(noise_array))     # Normalize\n",
    "  noise_array = 2*noise_array - 1     # Scale between -1 and 1\n",
    "\n",
    "  return noise_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING: Noisy Mel Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = [\"/data/custom_cough/noise_cough_augmented\", \"/data/custom_cough/noise_no_cough_augmented\"]\n",
    "dest_path = \"/data/custom_cough/mels_w_noise\"\n",
    "\n",
    "\n",
    "custom_path_dest = \"/custom_cough/noise_cough_augmented\"\n",
    "custom_path = \"/data/custom_cough/cough\"\n",
    "\n",
    "for wav in os.listdir(custom_path):\n",
    "    y, sr = librosa.load(os.path.join(custom_path, wav), sr=None)\n",
    "    brown_noise = generate_pink_noise(2, len(y))\n",
    "    pink_noise = generate_pink_noise(1, len(y))\n",
    "    mixed_noise = brown_noise*pink_noise\n",
    "    \n",
    "    flipmulti = np.random.multinomial(1, pvals=[1/3, 1/3, 1/3])\n",
    "\n",
    "    if flipmulti[0] == 1:\n",
    "        y_noise_injected = y + mixed_noise  # Additive mixed noise. Can also have a multiplicative noise like below but for the sake of demonstration keep it like this\n",
    "    elif flipmulti[1] == 1:\n",
    "        y_noise_injected = y + (pink_noise*y)   # Multiplicative pink noise (decaying noise)\n",
    "    else: \n",
    "        y_noise_injected = y + (brown_noise*y)  # Multiplicative brown noise (decaying soft noise)\n",
    "    \n",
    "    new_fname = \"noise\" + \"_\" + wav\n",
    "    \n",
    "    sf.write(os.path.join(custom_path_dest, new_fname), y_noise_injected, samplerate=sr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meller = MelSpectrogram(16000, n_fft=1024, hop_length=512, n_mels=64)\n",
    "\n",
    "for group in dataframe[\"label\"].unique():\n",
    "    data = dataframe.filter(pl.col(\"label\") == group)\n",
    "    \n",
    "    for row in data[\"id\"]:\n",
    "        if group == 1:\n",
    "            fullpath = os.path.join(wav_paths[0], row)\n",
    "        else:\n",
    "            fullpath = os.path.join(wav_paths[1], row)\n",
    "\n",
    "        y, sr = librosa.load(fullpath, sr=None)\n",
    "        y = cut_when_needed(y)\n",
    "        y = stretch_when_needed(y)\n",
    "        melconvert = meller(torch.tensor(y))\n",
    "        newname = row[:-4] + \".pt\"\n",
    "        torch.save(melconvert, os.path.join(dest_path, newname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING: Takens Embeddings - Delayed copies of a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = [\"/data/custom_cough/cough\", \"/data/custom_cough/noise_cough_augmented\", \n",
    "             \n",
    "             \"/data/custom_cough/no_cough\", \"/data/custom_cough/noise_no_cough_augmented\"]\n",
    "\n",
    "tks = SingleTakensEmbedding(time_delay=1, dimension=100, stride=30, n_jobs=-1, parameters_type=\"fixed\")\n",
    "\n",
    "dir_path = \"/data/custom_cough/topological_signal\"\n",
    "\n",
    "for group in dataframe[\"label\"].unique():\n",
    "    data = dataframe.filter(pl.col(\"label\") == group)\n",
    "    paths_check = wav_paths[:2] if group == 1 else wav_paths[2:]\n",
    "    \n",
    "    \n",
    "    for row in data[\"id\"]:\n",
    "        for path in paths_check:\n",
    "            fullpath = os.path.join(path, row)\n",
    "            \n",
    "            if os.path.exists(fullpath):\n",
    "                y, sr = librosa.load(fullpath, sr=None)\n",
    "                y = cut_when_needed(y)\n",
    "                y = stretch_when_needed(y)\n",
    "                y = tks.fit_transform(y)\n",
    "                \n",
    "                newname = row[:-4] + \".pt\"\n",
    "                \n",
    "                torch.save(y, os.path.join(dir_path, newname))\n",
    "                \n",
    "                break\n",
    "        \n",
    "\n",
    "# Doing likewise for testset (I use the words in opposite)\n",
    "dir_path = \"C:/Users/jako/data/custom_cough/topological_signal_test\"\n",
    "for group in dataframe[\"label\"].unique():\n",
    "    data = dataframe.filter(pl.col(\"label\") == group)\n",
    "    paths_check = wav_paths[:2] if group == 1 else wav_paths[2:]\n",
    "    \n",
    "    \n",
    "    for row in data[\"id\"]:\n",
    "        for path in paths_check:\n",
    "            fullpath = os.path.join(path, row)\n",
    "            \n",
    "            if os.path.exists(fullpath):\n",
    "                y, sr = librosa.load(fullpath, sr=None)\n",
    "                y = cut_when_needed(y)\n",
    "                y = stretch_when_needed(y)\n",
    "                y = tks.fit_transform(y)\n",
    "                \n",
    "                newname = row[:-4] + \".pt\"\n",
    "                \n",
    "                torch.save(y, os.path.join(dir_path, newname))\n",
    "                \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING: 3D Point Clouds of topological transforms\n",
    "#### See *parse_tensors.py* for the full structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing we do is to take the Takens embeddings\n",
    "tf = SingleTakensEmbedding(time_delay=1, dimension=100, stride=1, n_jobs=-1, parameters_type=\"fixed\") # Must be set to \"fixed\" and n_jobs=-1 for fast computation\n",
    "y_no = tf.fit_transform(y_no_cough)\n",
    "y_c = tf.fit_transform(y_cough )\n",
    "\n",
    "# Then do PCA with 3 components such that we have a 3D space\n",
    "pca = PCA(n_components=3)\n",
    "y_no_pca = pca.fit_transform(y_no)\n",
    "y_c_pca = pca.fit_transform(y_c)\n",
    "\n",
    "\n",
    "# Now we want to slice the 3D space into 24 frames by size 24x24 matrices (check the topology notebooks for cool viz!)\n",
    "def create_3d_tensor_from_pca(pca_data, shape=(24, 24, 24)):\n",
    "    # Determine the range for each axis\n",
    "    mins = pca_data.min(axis=0)\n",
    "    maxs = pca_data.max(axis=0)\n",
    "    \n",
    "    # Create the 3D tensor filled with zeros\n",
    "    tensor_3d = np.zeros(shape)\n",
    "    \n",
    "    # Determine the step size for each dimension\n",
    "    step_sizes = (maxs - mins) / np.array(shape)\n",
    "    \n",
    "    # Distribute the PCA points into the tensor\n",
    "    for point in pca_data:\n",
    "        # Calculate the voxel's index for each point\n",
    "        idx = ((point - mins) / step_sizes).astype(int)\n",
    "        \n",
    "        # Clip to ensure we don't exceed the shape due to floating point inaccuracies\n",
    "        idx = np.clip(idx, 0, np.array(shape) - 1)\n",
    "        \n",
    "        # Increment the voxel where the point falls\n",
    "        tensor_3d[tuple(idx)] += 1\n",
    "    \n",
    "    return tensor_3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA CONVERSION: Converting .webm to .wav \n",
    "See *convert_webm_to_wav.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_webm_to_wav(webm):\n",
    "    if webm.endswith(\".webm\"):\n",
    "        WEBM_FILE = os.path.join(coughvid_from_kaggle_path, webm)\n",
    "        \n",
    "        # Check the file size\n",
    "        if os.path.getsize(WEBM_FILE) > MAX_SIZE:\n",
    "            return f\"Skipping {webm} because it's larger than 100 kB.\"\n",
    "        \n",
    "        output_name = os.path.join(output_path, webm[:-5] + \".wav\")\n",
    "        \n",
    "        command = [\n",
    "            \"ffmpeg\", \"-i\", WEBM_FILE,\n",
    "            \"-vn\",\n",
    "            \"-acodec\", \"pcm_s16le\",  # 16-bit depth\n",
    "            \"-ac\", \"1\",  # Mono\n",
    "            \"-ar\", \"16000\",  # 44.1 kHz sample rate\n",
    "            \"-f\", \"wav\", \n",
    "            output_name\n",
    "        ]\n",
    "        try:\n",
    "            subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            return f\"Successfully converted {webm} to {output_name}\"\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return f\"Error processing {webm}: {e.stderr.decode('utf-8')}\"\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    webm_files = [f for f in os.listdir(kaggle_path) if f.endswith(\".webm\")]\n",
    "\n",
    "    with Pool(processes=cpu_count() // 2) as pool:  # Using half the CPU cores\n",
    "        results = pool.map(convert_webm_to_wav, webm_files)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagimob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
